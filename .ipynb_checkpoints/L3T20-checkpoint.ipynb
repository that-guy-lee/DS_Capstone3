{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:03.102409Z",
     "iopub.status.busy": "2022-08-31T11:07:03.101828Z",
     "iopub.status.idle": "2022-08-31T11:07:05.455146Z",
     "shell.execute_reply": "2022-08-31T11:07:05.453979Z",
     "shell.execute_reply.started": "2022-08-31T11:07:03.102357Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18280\\107568361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSpatialDropout1D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Embedding, SpatialDropout1D, BatchNormalization\n",
    "import wordcloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:05.456925Z",
     "iopub.status.busy": "2022-08-31T11:07:05.456657Z",
     "iopub.status.idle": "2022-08-31T11:07:05.461428Z",
     "shell.execute_reply": "2022-08-31T11:07:05.460384Z",
     "shell.execute_reply.started": "2022-08-31T11:07:05.456892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make results reproducible - set random seed\n",
    "from numpy.random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:05.463383Z",
     "iopub.status.busy": "2022-08-31T11:07:05.463025Z",
     "iopub.status.idle": "2022-08-31T11:07:05.476003Z",
     "shell.execute_reply": "2022-08-31T11:07:05.475190Z",
     "shell.execute_reply.started": "2022-08-31T11:07:05.463332Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_file = \"negative.txt\"\n",
    "positive_file = \"positive.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:05.478777Z",
     "iopub.status.busy": "2022-08-31T11:07:05.478178Z",
     "iopub.status.idle": "2022-08-31T11:07:05.489747Z",
     "shell.execute_reply": "2022-08-31T11:07:05.488900Z",
     "shell.execute_reply.started": "2022-08-31T11:07:05.478742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify - helper function to load and preprocess data\n",
    "def filter_words(line):    \n",
    "    line = re.sub(r'[^\\w\\s]','',line.rstrip())\n",
    "    words = line.split(\" \") \n",
    "    words = [i.lower() for i in words if i]      \n",
    "    return \" \".join(words)\n",
    "\n",
    "def load_data(filename):\n",
    "    thefile = open(filename, 'r') \n",
    "    lines = thefile.readlines() \n",
    "\n",
    "    data = []\n",
    "    for l in range(0,len(lines)): \n",
    "        if(lines[l-1].strip() == \"<title>\"): \n",
    "            theline = filter_words(lines[l])\n",
    "            if(len(theline) < 50):\n",
    "                data.append(theline)            \n",
    "            \n",
    "    return data\n",
    "\n",
    "# Helper function to convert categorical data to class label\n",
    "def to_word_label(y):\n",
    "    y = to_class(y)   \n",
    "    return [\"positive\" if i==0 else \"negative\" for i in y]\n",
    "\n",
    "# Helper function to convert class label to numeric label\n",
    "def to_numeric_label(y):\n",
    "  return [0 if i==\"positive\" else 1 for i in word_labels]\n",
    "\n",
    "# Helper function: this function needs to be called before sending arrays to sklearn metrics,\n",
    "# it converts back to class form from categorical form. ie: [1,0] --> 0, [0,1] --> 1\n",
    "def to_class(y):\n",
    "    return np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:05.491372Z",
     "iopub.status.busy": "2022-08-31T11:07:05.491092Z",
     "iopub.status.idle": "2022-08-31T11:07:05.546736Z",
     "shell.execute_reply": "2022-08-31T11:07:05.545552Z",
     "shell.execute_reply.started": "2022-08-31T11:07:05.491341Z"
    }
   },
   "outputs": [],
   "source": [
    "positive = load_data(positive_file)\n",
    "negative = load_data(negative_file)\n",
    "\n",
    "#to view the first 10 positive review text and first 10 negative reviews\n",
    "print(positive[0:10])\n",
    "print(\"\\n\")\n",
    "print(negative[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:05.548880Z",
     "iopub.status.busy": "2022-08-31T11:07:05.548612Z",
     "iopub.status.idle": "2022-08-31T11:07:05.556430Z",
     "shell.execute_reply": "2022-08-31T11:07:05.555519Z",
     "shell.execute_reply.started": "2022-08-31T11:07:05.548848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify - Combines the positive and negative reviews into a single list and create labels\n",
    "data = positive + negative\n",
    "word_labels = [\"positive\"] * len(positive) + [\"negative\"] * len(negative) \n",
    "\n",
    "# Converts labels to numbers in one-hot encoding - [1, 0] (positive) or [0, 1] (negative)\n",
    "# from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "labels  = to_categorical(to_numeric_label(word_labels))\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:05.558649Z",
     "iopub.status.busy": "2022-08-31T11:07:05.558028Z",
     "iopub.status.idle": "2022-08-31T11:07:07.655609Z",
     "shell.execute_reply": "2022-08-31T11:07:07.654699Z",
     "shell.execute_reply.started": "2022-08-31T11:07:05.558604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write some code to investigate the dataset. \n",
    "# - Calculate and report the mean review size, its standard deviation and create a boxplot.\n",
    "\n",
    "#already loaded the matplotlib libraries at the top\n",
    "\n",
    "count_words = []\n",
    "count = 0\n",
    "\n",
    "for i in data:\n",
    "    word_list = i.split()\n",
    "    number_of_words = len(word_list)\n",
    "    count_words.append(number_of_words)\n",
    "#     count_words += number_of_words\n",
    "    count += 1\n",
    "\n",
    "#Adding the words together\n",
    "add = sum(count_words)\n",
    "avg = add/count\n",
    "print('The mean review word size is : {:.1f}'.format(avg))\n",
    "\n",
    "std = np.std(count_words)\n",
    "print(\"The standard deviation is calculated to: {:.1f}\".format(std))\n",
    "\n",
    "plt.boxplot(count_words)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# - Calculate the number of unique words in the dataset\n",
    "\n",
    "# A set is a collection in which all elements are unique.\n",
    "unique_words = set(data)\n",
    "\n",
    "unique_word_count = len(unique_words)\n",
    "# display the number of unique words \n",
    "print(\"The number of unique words was calculated to be: {}\".format(unique_word_count))\n",
    "\n",
    "# - Perform any other dataset investigation that you feel would be valuable\n",
    "\n",
    "#plot a wordcloud of positive and negative book reviews  \n",
    "\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate(\" \".join(data))\n",
    "print(\"WORD CLOUD DIAGRAM\")\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"your_file_name\"+\".png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The word cloud above displays some of the most frequent words found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:07.657816Z",
     "iopub.status.busy": "2022-08-31T11:07:07.657055Z",
     "iopub.status.idle": "2022-08-31T11:07:07.712577Z",
     "shell.execute_reply": "2022-08-31T11:07:07.711934Z",
     "shell.execute_reply.started": "2022-08-31T11:07:07.657769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify - Tokenize the vocabulary \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=25)\n",
    "\n",
    "tokenizer.fit_on_texts(data) #create the vocabularry\n",
    "\n",
    "tokenized_data = tokenizer.texts_to_sequences(data) #tokenize the data using the vocabulary\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "\n",
    "# Compare a sample of the data before and after tokenization\n",
    "print(data[0:5])\n",
    "print(tokenized_data[0:5])\n",
    "#display the length of the tokenized data\n",
    "print(len(tokenized_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:07.714213Z",
     "iopub.status.busy": "2022-08-31T11:07:07.713547Z",
     "iopub.status.idle": "2022-08-31T11:07:07.733192Z",
     "shell.execute_reply": "2022-08-31T11:07:07.732179Z",
     "shell.execute_reply.started": "2022-08-31T11:07:07.714176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# Write some code to pre-process the data so that each review is the same length\n",
    "# Put the padding at the end of the sequences\n",
    "\n",
    "padded = pad_sequences(tokenized_data, maxlen = 4, padding = \"post\")\n",
    "\n",
    "#display the first five padded sequence\n",
    "print(\"Padded tokenized_data:\\n {}\".format(padded[ :5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:07.734990Z",
     "iopub.status.busy": "2022-08-31T11:07:07.734584Z",
     "iopub.status.idle": "2022-08-31T11:07:07.746997Z",
     "shell.execute_reply": "2022-08-31T11:07:07.745944Z",
     "shell.execute_reply.started": "2022-08-31T11:07:07.734944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write some code to split the data into a training and test set. Make sure you shuffle the data. Use 20% for the test set.\n",
    "y = labels\n",
    "X = padded\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:07.748967Z",
     "iopub.status.busy": "2022-08-31T11:07:07.748117Z",
     "iopub.status.idle": "2022-08-31T11:07:07.760096Z",
     "shell.execute_reply": "2022-08-31T11:07:07.759194Z",
     "shell.execute_reply.started": "2022-08-31T11:07:07.748928Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fill in the following function so it\n",
    "# - makes a prediction for the test set given the model\n",
    "# - reports the precision, recall and f1 score. Also print the confusion matrix. \n",
    "# You will need to use the helper to_class function to convert y_pred and y_test before supplying them to the sklearn functions.\n",
    "\n",
    "def assess_model(model, X_test, y_test):\n",
    "    \n",
    "    # use the helper to_class function to convert\n",
    "    # y_pred and y_test before supplying them to the sklearn functions.\n",
    "    y_test = to_class(y_test)\n",
    "    \n",
    "    #Get the predictions for y_pred\n",
    "    #makes a prediction for the test set given the model\n",
    "    y_pred = model.predict(X_test)\n",
    "       \n",
    "    # Calculate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #Evaluate precision_score\n",
    "    print()\n",
    "    print(precision_score(y_test, y_pred))\n",
    "    #Display the recall_score\n",
    "    print()\n",
    "    print(recall_score(y_test, y_pred))\n",
    "    #Display the f1_score\n",
    "    print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:07.762110Z",
     "iopub.status.busy": "2022-08-31T11:07:07.761600Z",
     "iopub.status.idle": "2022-08-31T11:07:08.062785Z",
     "shell.execute_reply": "2022-08-31T11:07:08.061782Z",
     "shell.execute_reply.started": "2022-08-31T11:07:07.762056Z"
    }
   },
   "outputs": [],
   "source": [
    "# initializing an object from TensorFlow’s Sequential class\n",
    "# Build the sequencial model of recurrent neural network (RNN)\n",
    "model = Sequential()\n",
    "\n",
    "# output_dim can be any value\n",
    "# output_dim is defined as the number of dimensions we wish to embed into\n",
    "#creating and adding the embedding layer\n",
    "embedding_layer = Embedding(input_dim = 1794, output_dim = 32, input_length = 4)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "\n",
    "# adding SpatialDropout1D(0.2) layer\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# BatchNormalization()\n",
    "BatchNormalization()\n",
    "\n",
    "# add an lstm layer of 32\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# adding a dense layer\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "#### TRAIN AND TUNE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:08.066047Z",
     "iopub.status.busy": "2022-08-31T11:07:08.065768Z",
     "iopub.status.idle": "2022-08-31T11:07:34.202561Z",
     "shell.execute_reply": "2022-08-31T11:07:34.201654Z",
     "shell.execute_reply.started": "2022-08-31T11:07:08.066007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compiling our neural network\n",
    "# The compilation step of building a neural network is where we specify the neural net’s optimizer and loss function.\n",
    "# model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "\n",
    "# Fitting The Recurrent Neural Network On The Training Set\n",
    "# Will specify epochs = 100 in this case.\n",
    "# the size of batches (32) that the network will be trained in through each epoch.\n",
    "# model.fit(X_train, y_train, output_dim = 10, epochs = 5, batch_size = 10)\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "# function to determine the best output_dim\n",
    "# creating an empty list\n",
    "empty_list = []\n",
    "\n",
    "lst = [10, 25, 50, 100]\n",
    "for i in lst:\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    model = Sequential()\n",
    "#     adding layers to the model\n",
    "    embedding_layer = Embedding(input_dim = 1794, output_dim = i, input_length = 4)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    BatchNormalization()\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "    history = model.fit(X_train, y_train, epochs = 5, batch_size = 10, validation_steps = 30)\n",
    "    model.summary()\n",
    "    \n",
    "    empty_list.append(model)\n",
    "    \n",
    "\n",
    "loss_value_model = empty_list[0] \n",
    "\n",
    "for model_loss in empty_list:\n",
    "    \n",
    "    if loss_value_model.loss > model_loss.loss:\n",
    "        \n",
    "        loss_value_model = model_loss\n",
    "        \n",
    "\n",
    "print(\"\\n\\n\\n\")        \n",
    "loss_value_model.summary()    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine performance of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot graphs for accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:34.206465Z",
     "iopub.status.busy": "2022-08-31T11:07:34.206209Z",
     "iopub.status.idle": "2022-08-31T11:07:34.435622Z",
     "shell.execute_reply": "2022-08-31T11:07:34.434727Z",
     "shell.execute_reply.started": "2022-08-31T11:07:34.206422Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the loss_per_epoch\n",
    "loss_per_epoch = model.history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)), loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:34.436905Z",
     "iopub.status.busy": "2022-08-31T11:07:34.436669Z",
     "iopub.status.idle": "2022-08-31T11:07:34.441465Z",
     "shell.execute_reply": "2022-08-31T11:07:34.440562Z",
     "shell.execute_reply.started": "2022-08-31T11:07:34.436877Z"
    }
   },
   "outputs": [],
   "source": [
    "# calling the assess_model function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:34.443805Z",
     "iopub.status.busy": "2022-08-31T11:07:34.442970Z",
     "iopub.status.idle": "2022-08-31T11:07:34.975481Z",
     "shell.execute_reply": "2022-08-31T11:07:34.974787Z",
     "shell.execute_reply.started": "2022-08-31T11:07:34.443758Z"
    }
   },
   "outputs": [],
   "source": [
    "#display the first 10 model predictions\n",
    "predictins = model.predict(X_test)\n",
    "predictins[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-31T11:07:34.977094Z",
     "iopub.status.busy": "2022-08-31T11:07:34.976849Z",
     "iopub.status.idle": "2022-08-31T11:07:34.986898Z",
     "shell.execute_reply": "2022-08-31T11:07:34.986121Z",
     "shell.execute_reply.started": "2022-08-31T11:07:34.977063Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is a very small set of completed new data to use to make predictions.\n",
    "prediction_data = [\"this book is fabulous\",\"i hated this book\", \"the best\", \"no good\", \"okay\"]\n",
    "tokenized = tokenizer.texts_to_sequences(prediction_data)\n",
    "padded = pad_sequences(tokenized, padding = 'post', maxlen = 4)\n",
    "\n",
    "# Supply this data to each of your models and see how it does. \n",
    "# You can call the helper function \"to_word_label\" to map the output of the model to the name of the\n",
    "# class it was predicted to belong to.\n",
    "\n",
    "#call filter_words function to load and preprocess data\n",
    "#maxlen was changed to 4 \n",
    "\n",
    "#display the predicted results of the model\n",
    "print(to_word_label(padded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] https://www.freecodecamp.org/news/the-ultimate-guide-to-recurrent-neural-networks-in-python7/\n",
    "\n",
    "[2] https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn\n",
    "\n",
    "[3] https://www.freecodecamp.org/news/the-ultimate-guide-to-recurrent-neural-networks-in-python/\n",
    "\n",
    "[4] https://python.tutorialink.com/generating-word-cloud-for-items-in-a-list-in-python/\n",
    "\n",
    "[5] https://www.youtube.com/watch?v=jC0jXsX-33Q\n",
    "\n",
    "[6] https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n",
    "\n",
    "[7] https://www.youtube.com/watch?v=S8tpSG6Q2H0\n",
    "\n",
    "[8] https://towardsdatascience.com/step-by-step-guide-building-a-prediction-model-in-python-ac441e8b9e8b\n",
    "\n",
    "[9] https://www.nbshare.io/notebook/249468051/How-To-Code-RNN-and-LSTM-Neural-Networks-in-Python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
